{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "import math\n",
    "import numpy\n",
    "from datetime import datetime\n",
    "from shapely import wkt\n",
    "from shapely.errors import WKTReadingError\n",
    "from shapely.geometry import mapping, shape\n",
    "from shapely import Polygon\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "login = {\n",
    "    \"username\":\"Dorito\",\n",
    "    \"password\":\"Portador123\"\n",
    "}\n",
    "# Crear una sesi√≥n para mantener la cookie\n",
    "session = requests.Session()\n",
    "response = session.post(\"https://192.168.179.3/api/user/login/\", login, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_paginated(url):\n",
    "    login = {\n",
    "        \"username\":\"Dorito\",\n",
    "        \"password\":\"Portador123\"\n",
    "    }\n",
    "    # Crear una sesi√≥n para mantener la cookie\n",
    "    session = requests.Session()\n",
    "    response = session.post(\"https://192.168.179.3/api/user/login/\", login, verify=False)\n",
    "    datos = []\n",
    "    while url:\n",
    "        response = session.get(url, verify=False)\n",
    "        info = response.json()\n",
    "        datos.extend(info[\"results\"])\n",
    "        url = info[\"next\"]\n",
    "    return datos\n",
    "\n",
    "def get_data_not_paginated(url):\n",
    "    login = {\n",
    "        \"username\":\"Dorito\",\n",
    "        \"password\":\"Portador123\"\n",
    "    }\n",
    "    # Crear una sesi√≥n para mantener la cookie\n",
    "    session = requests.Session()\n",
    "    response = session.post(\"https://192.168.179.3/api/user/login/\", login, verify=False)\n",
    "    response = session.get(url, verify=False)\n",
    "    info = response.json()\n",
    "    return info\n",
    "\n",
    "\n",
    "def parse_geom_safe(wkt_string):\n",
    "    try:\n",
    "        if isinstance(wkt_string, str) and \"SRID=\" in wkt_string:\n",
    "            return wkt.loads(wkt_string.split(\";\", 1)[1])\n",
    "    except:\n",
    "        pass\n",
    "    return None  # simplemente ignora lo inv√°lido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener y normalizar datos\n",
    "url = \"https://192.168.179.3/api/v1/Extensionista/\"\n",
    "print(\"cargando datos lotes\")\n",
    "datos = get_data_not_paginated(url)\n",
    "df = pd.json_normalize(datos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "<h2>EXPORTANDO PRODUCTORES</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_productor = \"https://192.168.179.3/api/v1/productor/\"\n",
    "url_productor_2 = \"https://192.168.179.3/api/v1/productor/mongo_get/productor_data/\"\n",
    "\n",
    "print(\"cargando datos productor\")\n",
    "datos_productor = get_data_not_paginated(url_productor)\n",
    "print(\"cargando datos productor\")\n",
    "datos_productor_2 = get_data_not_paginated(url_productor_2)\n",
    "\n",
    "df_productor = pd.json_normalize(datos_productor)\n",
    "df_productor_2 = pd.json_normalize(datos_productor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar productores que tienen postgres_data.id\n",
    "df_productor = df_productor[df_productor[\"id\"].notna()]\n",
    "# Realizar los merge sin eliminar filas del DataFrame principal\n",
    "df_completo = df_productor.merge(\n",
    "    df_productor_2,\n",
    "    left_on=\"productor_data\",\n",
    "    right_on=\"_id\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "<h2>CONSTRUYENDO LOS LOTES</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener y normalizar datos\n",
    "url = \"https://192.168.179.3/api/v1/poligonos_lotes/\"\n",
    "url_productor = \"https://192.168.179.3/api/v1/productor/\"\n",
    "url_productor_2 = \"https://192.168.179.3/api/v1/productor/mongo_get/productor_data/\"\n",
    "url_fincas = \"https://192.168.179.3/api/v1/poligonos_fincas/\"\n",
    "url_lotes = \"https://192.168.179.3/api/v1/poligonos_lotes/mongo_get/mongo_atribute/\"\n",
    "print(\"cargando datos lotes\")\n",
    "datos = get_data_paginated(url)\n",
    "print(\"cargando datos productor\")\n",
    "datos_productor = get_data_not_paginated(url_productor)\n",
    "print(\"cargando datos productor\")\n",
    "datos_productor_2 = get_data_not_paginated(url_productor_2)\n",
    "print(\"cargando datos fincas\")\n",
    "datos_fincas = get_data_paginated(url_fincas)\n",
    "print(\"cargando datos lotes\")\n",
    "datos_lotes = get_data_not_paginated(url_lotes)\n",
    "\n",
    "df = pd.json_normalize(datos)\n",
    "df_productor = pd.json_normalize(datos_productor)\n",
    "df_productor_2 = pd.json_normalize(datos_productor_2)\n",
    "df_fincas = pd.json_normalize(datos_fincas)\n",
    "df_lotes = pd.json_normalize(datos_lotes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar productores que tienen postgres_data.id\n",
    "df_productor = df_productor[df_productor[\"id\"].notna()]\n",
    "# Realizar los merge sin eliminar filas del DataFrame principal\n",
    "df_completo = df.merge(\n",
    "    df_productor,\n",
    "    left_on=\"productor\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_completo = df_completo.merge(\n",
    "    df_productor_2,\n",
    "    left_on=\"productor\",\n",
    "    right_on=\"postgres_data.id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_completo = df_completo.merge(\n",
    "    df_fincas,\n",
    "    left_on=\"finca\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "df_completo = df_completo.merge(\n",
    "    df_lotes,\n",
    "    left_on=\"id_x\",\n",
    "    right_on=\"poligono_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(df_completo.columns)\n",
    "# Aplicar solo a las v√°lidas\n",
    "df_completo[\"geometry\"] = df_completo[\"poligono_x\"].apply(parse_geom_safe)\n",
    "\n",
    "# Crear el GeoDataFrame sin que explote\n",
    "gdf = gpd.GeoDataFrame(df_completo, geometry=\"geometry\", crs=\"EPSG:4326\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnas que s√≠ quieres conservar\n",
    "cols_keep = [\n",
    "    'id_x', 'area_x', 'deshabilitar_poligono_x', 'documento','datos.nombre_completo', 'extensionista',\n",
    "    'datos.numero_documento', 'datos.numero_lote', 'datos.fecha_visita',\n",
    "    'datos.categoria', 'datos.subtipo_operacion', 'datos.fecha_actividad',\n",
    "    'datos.variedad', 'datos.produccion', 'datos.estado_cultivo',\n",
    "    'datos.distancia_surcos', 'datos.distancia_plantas', 'datos.densidad',\n",
    "    'datos.numero_plantas', 'datos.gramos_plantas', 'datos.kg_produccion',\n",
    "    'datos.descripcion', 'datos.observacion', 'geometry'\n",
    "]\n",
    "\n",
    "# filtrar el GeoDataFrame\n",
    "gdf = gdf[cols_keep].drop_duplicates(subset=[\"id_x\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns = [\"id\", \"area\", \"deshabilitar_poligono\", \"documento\", \"nom_prod\", \"extensionista\", \"numero_documento\", \"numero_lote\", \"fecha_visita\", \n",
    "\"categoria\", \"subtipo_operacion\", \"fecha_actividad\", \"variedad\", \"produccion\", \"estado_cultivo\", \"distancia_surcos\", \"distancia_plantas\", \"densidad\",\n",
    "\"numero_plantas\", \"gramos_plantas\", \"kg_produccion\", \"descripcion\", \"observacion\", \"geometry\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "def boolean_to_int(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Convierte todas las columnas booleanas de un GeoDataFrame\n",
    "    a valores enteros (0 y 1).\n",
    "    \"\"\"\n",
    "    gdf_copy = gdf.copy()\n",
    "    for col in gdf_copy.select_dtypes(include=[\"bool\"]).columns:\n",
    "        gdf_copy[col] = gdf_copy[col].astype(int)\n",
    "    return gdf_copy\n",
    "gdf = boolean_to_int(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[\"documento\"] = gdf[\"documento\"].astype(\"string\")\n",
    "gdf[\"documento\"] = (\n",
    "    gdf[\"documento\"]\n",
    "    .astype(str)                # todo a string\n",
    "    .str.replace(r\"\\.0$\", \"\", regex=True)  # quita el .0 final\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.sort_values(by='area', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_excel(r\"C:\\Users\\dorito\\Downloads\\productores_en_area_protegida.xlsx\", sheet_name='Hoja1')\n",
    "# LISTA_CEDULA = df[\"cedula\"].to_list()\n",
    "# LISTA_CEDULA = [str(cedula) for cedula in LISTA_CEDULA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[\"fecha_visita\"] = pd.to_datetime(gdf[\"fecha_visita\"], errors=\"coerce\")\n",
    "gdf[\"fecha_actividad\"] = pd.to_datetime(gdf[\"fecha_actividad\"], errors=\"coerce\")\n",
    "gdf[\"doc_prod\"] = None\n",
    "gdf[\"actualizar\"] = 0\n",
    "gdf[\"otro_asoc\"] = 0\n",
    "gdf[\"calcular_p\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.drop_duplicates(subset=[\"geometry\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(\"../Lote g.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "<H2>ARREGLANDO LAS FINCAS</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener y normalizar datos\n",
    "url = \"https://192.168.179.3/api/v1/poligonos_fincas/\"\n",
    "url_productor = \"https://192.168.179.3/api/v1/productor/\"\n",
    "url_productor_2 = \"https://192.168.179.3/api/v1/productor/mongo_get/productor_data/\"\n",
    "url_lotes = \"https://192.168.179.3/api/v1/poligonos_fincas/mongo_get/mongo_atribute/\"\n",
    "print(\"cargando datos fincas\")\n",
    "datos = get_data_paginated(url)\n",
    "print(\"cargando datos productor\")\n",
    "datos_productor = get_data_not_paginated(url_productor)\n",
    "print(\"cargando datos productor_2\")\n",
    "datos_productor_2 = get_data_not_paginated(url_productor_2)\n",
    "print(\"cargando datos fincas atributos\")\n",
    "datos_lotes = get_data_not_paginated(url_lotes)\n",
    "\n",
    "df = pd.json_normalize(datos)\n",
    "df_productor = pd.json_normalize(datos_productor)\n",
    "df_productor_2 = pd.json_normalize(datos_productor_2)\n",
    "df_lotes = pd.json_normalize(datos_lotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar productores que tienen postgres_data.id\n",
    "df_productor = df_productor[df_productor[\"id\"].notna()]\n",
    "# Realizar los merge sin eliminar filas del DataFrame principal\n",
    "df_completo = df.merge(\n",
    "    df_productor,\n",
    "    left_on=\"productor\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_completo = df_completo.merge(\n",
    "    df_productor_2,\n",
    "    left_on=\"productor\",\n",
    "    right_on=\"postgres_data.id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_completo = df_completo.merge(\n",
    "    df_lotes,\n",
    "    left_on=\"id_x\",\n",
    "    right_on=\"poligono_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "# Aplicar solo a las v√°lidas\n",
    "df_completo[\"geometry\"] = df_completo[\"poligono\"].apply(parse_geom_safe)\n",
    "print(df_completo.columns)\n",
    "# Crear el GeoDataFrame sin que explote\n",
    "gdf_fincas = gpd.GeoDataFrame(df_completo, geometry=\"geometry\", crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnas que s√≠ quieres conservar\n",
    "cols_keep = ['id_x', 'area', 'deshabilitar_poligono', 'codigo_finca',\n",
    "'documento', 'extensionista', 'datos.nombre_completo','datos.nombre_finca',\n",
    "'datos.categoria', 'datos.fecha_visita','datos.descripcion', 'datos.observaciones', 'geometry']\n",
    "\n",
    "# filtrar el GeoDataFrame\n",
    "gdf_fincas = gdf_fincas[cols_keep].drop_duplicates(subset=[\"id_x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_fincas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_fincas.columns = ['id', 'area', 'deshabilitar_poligono', 'codigo_finca',\n",
    "'documento', 'extensionista', 'nom_prod', 'nombre_finca',\n",
    "'categoria', 'fecha_visita','descripcion', 'observaciones', 'geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_fincas = gdf_fincas.sort_values(by='area', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_fincas[\"fecha_visita\"] = pd.to_datetime(gdf_fincas[\"fecha_visita\"], errors=\"coerce\")\n",
    "gdf_fincas[\"doc_prod\"] = None\n",
    "gdf_fincas[\"actualizar\"] = 0\n",
    "gdf_fincas[\"otro_asoc\"] = 0\n",
    "gdf_fincas[\"calcular_p\"] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_fincas[\"documento\"] = gdf_fincas[\"documento\"].astype(\"string\")\n",
    "gdf_fincas[\"documento\"] = (\n",
    "    gdf_fincas[\"documento\"]\n",
    "    .astype(str)                # todo a string\n",
    "    .str.replace(r\"\\.0$\", \"\", regex=True)  # quita el .0 final\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_fincas[\"datos.fecha_visita\"] = pd.to_datetime(\n",
    "    gdf[\"datos.fecha_visita\"],\n",
    "    format=\"%Y-%m-%d\",\n",
    "    errors=\"coerce\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_fincas.to_file(\"../Finca.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "jhkjhkjkj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "<H3>POLIGONOS CONSERVACION</H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener y normalizar datos\n",
    "url = \"https://192.168.179.3/api/v1/poligonos_conservacion/\"\n",
    "url_productor = \"https://192.168.179.3/api/v1/productor/\"\n",
    "url_fincas = \"https://192.168.179.3/api/v1/poligonos_conservacion/\"\n",
    "print(\"cargando datos lotes\")\n",
    "datos = get_data_paginated(url)\n",
    "print(\"cargando datos productor\")\n",
    "datos_productor = get_data_not_paginated(url_productor)\n",
    "print(\"cargando datos fincas\")\n",
    "datos_fincas = get_data_paginated(url_fincas)\n",
    "\n",
    "df = pd.json_normalize(datos)\n",
    "df_productor = pd.json_normalize(datos_productor)\n",
    "df_fincas = pd.json_normalize(datos_fincas)\n",
    "# df_lotes = pd.json_normalize(datos_lotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar productores que tienen postgres_data.id\n",
    "df_productor = df_productor[df_productor[\"id\"].notna()]\n",
    "# Realizar los merge sin eliminar filas del DataFrame principal\n",
    "df_completo = df.merge(\n",
    "    df_productor,\n",
    "    left_on=\"productor\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# df_completo = df_completo.merge(\n",
    "#     df_lotes,\n",
    "#     left_on=\"id_x\",\n",
    "#     right_on=\"poligono_id\",\n",
    "#     how=\"left\"\n",
    "# )\n",
    "\n",
    "# Aplicar solo a las v√°lidas\n",
    "df_completo[\"geometry\"] = df_completo[\"poligono\"].apply(parse_geom_safe)\n",
    "\n",
    "# Crear el GeoDataFrame sin que explote\n",
    "gdf = gpd.GeoDataFrame(df_completo, geometry=\"geometry\", crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(\"./conservacion.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "<h3>Poligonos Infraestructura</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener y normalizar datos\n",
    "url = \"https://192.168.179.3/api/v1/poligonos_infraestructura/\"\n",
    "url_productor = \"https://192.168.179.3/api/v1/productor/\"\n",
    "url_fincas = \"https://192.168.179.3/api/v1/poligonos_infraestructura/\"\n",
    "print(\"cargando datos lotes\")\n",
    "datos = get_data_paginated(url)\n",
    "print(\"cargando datos productor\")\n",
    "datos_productor = get_data_not_paginated(url_productor)\n",
    "print(\"cargando datos fincas\")\n",
    "datos_fincas = get_data_paginated(url_fincas)\n",
    "\n",
    "df = pd.json_normalize(datos)\n",
    "df_productor = pd.json_normalize(datos_productor)\n",
    "df_fincas = pd.json_normalize(datos_fincas)\n",
    "# df_lotes = pd.json_normalize(datos_lotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar productores que tienen postgres_data.id\n",
    "df_productor = df_productor[df_productor[\"id\"].notna()]\n",
    "# Realizar los merge sin eliminar filas del DataFrame principal\n",
    "df_completo = df.merge(\n",
    "    df_productor,\n",
    "    left_on=\"productor\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# df_completo = df_completo.merge(\n",
    "#     df_lotes,\n",
    "#     left_on=\"id_x\",\n",
    "#     right_on=\"poligono_id\",\n",
    "#     how=\"left\"\n",
    "# )\n",
    "\n",
    "# Aplicar solo a las v√°lidas\n",
    "df_completo[\"geometry\"] = df_completo[\"poligono\"].apply(parse_geom_safe)\n",
    "\n",
    "# Crear el GeoDataFrame sin que explote\n",
    "gdf = gpd.GeoDataFrame(df_completo, geometry=\"geometry\", crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(\"./Infraestructura.shp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "<h3>Arreglando errores duplicados y asi</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "\n",
    "# def detectar_invalidos(gdf):\n",
    "#     # Caso 1: geometr√≠as None o vac√≠as\n",
    "#     vacios = gdf[gdf[\"geometry\"].is_empty | gdf[\"geometry\"].isna()]\n",
    "\n",
    "#     # Caso 2: geometr√≠as inv√°lidas\n",
    "#     invalidos = gdf[~gdf[\"geometry\"].is_valid]\n",
    "\n",
    "#     # Unir ambos\n",
    "#     resultado = pd.concat([vacios, invalidos]).drop_duplicates()\n",
    "\n",
    "#     return resultado\n",
    "\n",
    "# # Ejemplo de uso\n",
    "# gdf_invalidos = detectar_invalidos(gdf)\n",
    "# print(\"Cantidad de geometr√≠as vac√≠as o inv√°lidas:\", len(gdf_invalidos))\n",
    "\n",
    "# # Si quieres exportar para revisar en QGIS\n",
    "# if not gdf_invalidos.empty:\n",
    "#     gdf_invalidos.to_file(\"Poligonos_invalidos.shp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "\n",
    "# def detectar_solapados(gdf, umbral=0.8):\n",
    "#     # aseguramos que todo est√° en la misma proyecci√≥n m√©trica para calcular √°reas\n",
    "#     if gdf.crs.is_geographic:\n",
    "#         gdf = gdf.to_crs(3857)  \n",
    "\n",
    "#     overlaps = []\n",
    "\n",
    "#     for i, geom1 in gdf.iterrows():\n",
    "#         print(f\"{i}/{len(gdf)}\")\n",
    "#         for j, geom2 in gdf.iterrows():\n",
    "#             if i >= j:  # evitar comparar dos veces o consigo mismo\n",
    "#                 continue\n",
    "#             inter = geom1.geometry.intersection(geom2.geometry)\n",
    "\n",
    "#             if not inter.is_empty:\n",
    "#                 area_inter = inter.area\n",
    "#                 area1 = geom1.geometry.area\n",
    "#                 area2 = geom2.geometry.area\n",
    "\n",
    "#                 # porcentaje relativo de intersecci√≥n\n",
    "#                 porc1 = area_inter / area1\n",
    "#                 porc2 = area_inter / area2\n",
    "\n",
    "#                 if porc1 >= umbral or porc2 >= umbral:\n",
    "#                     overlaps.append({\n",
    "#                         \"id1\": i,\n",
    "#                         \"id2\": j,\n",
    "#                         \"area_inter\": area_inter,\n",
    "#                         \"porc1\": porc1,\n",
    "#                         \"porc2\": porc2\n",
    "#                     })\n",
    "\n",
    "#     return gpd.GeoDataFrame(overlaps)\n",
    "\n",
    "# # Ejemplo de uso\n",
    "# result = detectar_solapados(gdf, umbral=0.8)\n",
    "\n",
    "# print(result)\n",
    "# result.to_file(\"Lotes_a_revisar.shp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # sacar los IDs √∫nicos que participan en solapamientos\n",
    "# ids_solapados = set(result[\"id1\"]).union(set(result[\"id2\"]))\n",
    "\n",
    "# # filtrar el gdf original para quedarnos con solo los que se solapan\n",
    "# gdf_solapados = gdf.loc[gdf.index.isin(ids_solapados)]\n",
    "\n",
    "# print(f\"Se detectaron {len(ids_solapados)} pol√≠gonos solapados\")\n",
    "# gdf_solapados.to_file(\"Lotes_solapados.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf[\"fecha\"] = pd.to_datetime(\n",
    "#     gdf[\"fecha\"], \n",
    "#     format=\"%d/%m/%Y %H:%M:%S\", \n",
    "#     errors=\"coerce\"   # pone NaT si hay errores de formato\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_finca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_solapados_filtro.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_analizados = []\n",
    "# ids_eliminar = []\n",
    "\n",
    "# for _, row in result.iterrows():  # recorremos filas del DataFrame result\n",
    "#     print(f\"{_}/{len(result)}\")\n",
    "#     id_1 = row[\"id1\"]\n",
    "#     id_2 = row[\"id2\"]\n",
    "\n",
    "#     # si ya procesamos alguno de los dos, saltamos\n",
    "#     if id_1 in ids_analizados or id_2 in ids_analizados:\n",
    "#         continue  \n",
    "\n",
    "#     ids_analizados.append(id_1)\n",
    "#     ids_analizados.append(id_2)\n",
    "#     gdf_solapados_filtro = gdf.loc[gdf.index.isin([id_1, id_2])]\n",
    "#     union_gdf1 = gdf_solapados_filtro.unary_union    \n",
    "#     gdf_fincas_intersectadas = gdf_finca[gdf_finca.intersects(union_gdf1)]\n",
    "\n",
    "#     if not gdf_fincas_intersectadas.empty:\n",
    "#         lista_documentos = gdf_fincas_intersectadas[\"documento\"].to_list()\n",
    "#         gdf_solapados_filtro_exorcismo = gdf_solapados_filtro.loc[~gdf_solapados_filtro[\"documento\"].isin(lista_documentos)]\n",
    "#         if not gdf_solapados_filtro_exorcismo.empty:\n",
    "#             for idx in gdf_solapados_filtro_exorcismo[\"id_x\"]:\n",
    "#                 ids_eliminar.append(idx)\n",
    "#             continue\n",
    "\n",
    "\n",
    "#     gdf_mas_antiguo = gdf_solapados_filtro.loc[[gdf_solapados_filtro[\"fecha\"].idxmin()]]\n",
    "#     if not gdf_mas_antiguo.empty:\n",
    "#         ids_eliminar.append(gdf_mas_antiguo[\"id_x\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, id_eliminar in enumerate(ids_eliminar):\n",
    "#     print(f\"{i}/{len(ids_eliminar)}\")\n",
    "#     url = f\"https://192.168.179.3/api/v1/poligonos_lotes/{id_eliminar}/\"\n",
    "#     print(url)\n",
    "#     session.delete(url, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = gpd.read_file(r\"C:\\Users\\dorito\\Downloads\\trazabilidad.geojson\")\n",
    "# gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tomar el centroide del pol√≠gono\n",
    "# gdf[\"lon\"] = gdf.geometry.centroid.x\n",
    "# gdf[\"lat\"] = gdf.geometry.centroid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf.to_file(\"./trazabilidad.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener y normalizar datos\n",
    "url = \"https://192.168.179.3/api/v1/productor/\"\n",
    "print(\"cargando datos lotes\")\n",
    "datos = get_data_not_paginated(url)\n",
    "df = pd.json_normalize(datos)\n",
    "\n",
    "# Obtener y normalizar datos\n",
    "url = \"https://192.168.179.3/api/v1/productor/mongo_get/productor_data/\"\n",
    "print(\"cargando datos lotes\")\n",
    "datos = get_data_not_paginated(url)\n",
    "df_productor = pd.json_normalize(datos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"productor_data\"] = df[\"productor_data\"].astype(str)\n",
    "df_productor[\"postgres_data.id\"] = df_productor[\"postgres_data.id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado[\"productor_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtrado = df[df[\"productor_data\"].notna()].copy()\n",
    "df_total = df_filtrado.merge(\n",
    "    df_productor,\n",
    "    left_on=\"id\",\n",
    "    right_on=\"postgres_data.id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total[\"datos.fecha_afiliacion\"].str.startswith(\"2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025 = df_total[df_total[\"datos.fecha_afiliacion\"].str.startswith(\"2025\", na=False)].copy()\n",
    "\n",
    "print(f\"Filas originales: {len(df_total)}\")\n",
    "print(f\"Filas del a√±o 2025: {len(df_2025)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def quitar_tildes(texto):\n",
    "    \"\"\"Elimina tildes y caracteres especiales de un texto.\"\"\"\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "    return \"\".join(\n",
    "        c for c in unicodedata.normalize(\"NFD\", texto)\n",
    "        if unicodedata.category(c) != \"Mn\"\n",
    "    )\n",
    "j = 1\n",
    "for i, row in df_2025.iterrows():\n",
    "    print(f\"{j}/{len(df_2025)}\")\n",
    "    j += 1\n",
    "    if pd.isna(row[\"datos.nombre_completo\"]):\n",
    "        print(f\"‚ö†Ô∏è No se puede trabajar con {row['documento']}\")\n",
    "        continue\n",
    "    if pd.isna(row[\"documento\"]):\n",
    "        print(f\"‚ö†Ô∏è No tiene documento {row['id']}\")\n",
    "        continue\n",
    "\n",
    "    codigo_inicio = \"COOPR2025\"\n",
    "\n",
    "    # --- Procesar nombre ---\n",
    "    nombre = quitar_tildes(row[\"datos.nombre_completo\"]).upper()\n",
    "    iniciales = \"\".join([p[0] for p in nombre.split() if p])\n",
    "    print(row[\"documento\"])\n",
    "    # --- Procesar documento ---\n",
    "    documento = str(int(row[\"documento\"]))  # Por si viene como float o con .0\n",
    "    ultimos_3 = documento[-3:] if len(documento) >= 3 else documento.zfill(3)\n",
    "\n",
    "    # --- Crear c√≥digo final ---\n",
    "    codigo = f\"{codigo_inicio}{iniciales}{ultimos_3}\"\n",
    "    objeto = {\n",
    "        \"codigo_productor\": codigo\n",
    "    }\n",
    "\n",
    "    # df_actualizar.at[i, \"codigo_generado\"] = codigo\n",
    "    # print(f\"{nombre} ({documento}) ‚Üí {codigo}\")\n",
    "    session.patch(f\"https://192.168.179.3/api/v1/productor/{row['id']}\", json=objeto, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../temp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    url = f\"https://192.168.179.3/api/v1/productor/sql/?documento={row['CEDULA']}\"\n",
    "    respuesta = session.get(url, verify=False)\n",
    "    respuesta = respuesta.json()[0]\n",
    "    df.at[i, \"id\"] = respuesta[\"id\"]\n",
    "    df.at[i, \"codigo_productor\"] = respuesta[\"codigo_productor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    url = f\"https://192.168.179.3/api/v1/productor/{row['id']}/mongo_get/productor_data/\"\n",
    "    print(f\"\\nüîó URL: {url}\")\n",
    "\n",
    "    respuesta = {\"productor_data\":{\n",
    "        \"activo\": True,\n",
    "        \"delegado\": False,\n",
    "        \"nombre_completo\": row[\"NOMBRE \"],\n",
    "        \"fecha_afiliacion\": \"2025-09-01\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"üì§ Enviando datos:\", respuesta)\n",
    "\n",
    "    try:\n",
    "        r = session.post(url, json=respuesta, verify=False)\n",
    "        print(\"üì• Estado:\", r.status_code)\n",
    "        \n",
    "        # Mostrar el cuerpo de la respuesta\n",
    "        try:\n",
    "            print(\"üì¶ Respuesta JSON:\", r.json())\n",
    "        except ValueError:\n",
    "            print(\"üìÑ Respuesta texto:\", r.text)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"‚ùå Error de conexi√≥n:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_fotos = r\"C:\\Users\\dorito\\Downloads\"\n",
    "puntos_fotos = gpd.read_file(r\"C:\\Users\\dorito\\Documents\\Posicion Fotos Lotes.shp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "puntos_fotos[\"longitud\"] = puntos_fotos.geometry.x\n",
    "puntos_fotos[\"latitud\"] = puntos_fotos.geometry.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "puntos_fotos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "puntos_fotos[\"ruta\"] = puntos_fotos['Nombre Lot'].apply(lambda x: os.path.join(folder_fotos, f\"{x}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "<h2>ORGANIZANDO INFORMACION A IBARRA RELACION DE ASOCIADOS PROBLEMAS EN LA BASE</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "asociados_a_revisar = [4883329,4917242,12229437,12295051,14214331,79136928,79749725,83239785,83250337]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_revisar = gdf[[\"id_x\",\"documento\", \"datos.numero_lote\", \"datos.numero_plantas\"]].loc[gdf[\"documento\"].isin(asociados_a_revisar)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_revisar.loc[(gdf_revisar[\"datos.numero_plantas\"].isna())|(gdf_revisar[\"datos.numero_plantas\"] == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\dorito\\Downloads\\POLIGONOS COCENTRAL AULA MAYER VERIFICADOS.xlsx\")\n",
    "gdf_1 = gpd.read_file(\"./POLIGONOS TRAZABILIDAD 17 10 2025.xlsx/PRIMERA PARTE FALTANTE.geojson\")\n",
    "gdf_2 = gpd.read_file(\"./POLIGONOS TRAZABILIDAD 17 10 2025.xlsx/SEGUNDA PARTE FALTANTE.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_2 = gdf_2.rename(columns={\"CEDULA\": \"documento\"})\n",
    "gdf_2.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_total = gpd.GeoDataFrame(pd.concat([gdf_1,gdf_2]), crs=4326, geometry=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import unary_union\n",
    "gdfs = []\n",
    "for documento in gdf_total[\"documento\"].unique():\n",
    "    documento = int(documento)\n",
    "    df_filtro = df.loc[df[\"DOCUMENTO\"] == documento]\n",
    "    gdf_filtro = gdf_total.loc[gdf[\"documento\"] == str(documento)]\n",
    "    df_filtro[\"geometry\"] = unary_union(gdf_filtro.geometry)\n",
    "    gdfs.append(df_filtro)\n",
    "gdf_traza = gpd.GeoDataFrame(pd.concat(gdfs), geometry=\"geometry\", crs=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_traza = gdf_traza.drop(\n",
    "    columns=[col for col in gdf_traza.columns if \"unnamed\" in col.lower()]\n",
    ")\n",
    "gdf_traza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_traza.to_file(\"./Anexo_Poligonos_Laumayer.geojson\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
