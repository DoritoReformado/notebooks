{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "import math\n",
    "import numpy\n",
    "from datetime import datetime\n",
    "from shapely import wkt\n",
    "from shapely.errors import WKTReadingError\n",
    "from shapely.geometry import mapping, shape\n",
    "from shapely import Polygon\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "login = {\n",
    "    \"username\":\"Dorito\",\n",
    "    \"password\":\"Portador123\"\n",
    "}\n",
    "# Crear una sesión para mantener la cookie\n",
    "session = requests.Session()\n",
    "response = session.post(\"https://192.168.179.3/api/user/login/\", login, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_paginated(url):\n",
    "    login = {\n",
    "        \"username\":\"Dorito\",\n",
    "        \"password\":\"Portador123\"\n",
    "    }\n",
    "    # Crear una sesión para mantener la cookie\n",
    "    session = requests.Session()\n",
    "    response = session.post(\"https://192.168.179.3/api/user/login/\", login, verify=False)\n",
    "    datos = []\n",
    "    while url:\n",
    "        response = session.get(url, verify=False)\n",
    "        info = response.json()\n",
    "        datos.extend(info[\"results\"])\n",
    "        url = info[\"next\"]\n",
    "    return datos\n",
    "\n",
    "def get_data_not_paginated(url):\n",
    "    login = {\n",
    "        \"username\":\"Dorito\",\n",
    "        \"password\":\"Portador123\"\n",
    "    }\n",
    "    # Crear una sesión para mantener la cookie\n",
    "    session = requests.Session()\n",
    "    response = session.post(\"https://192.168.179.3/api/user/login/\", login, verify=False)\n",
    "    response = session.get(url, verify=False)\n",
    "    info = response.json()\n",
    "    return info\n",
    "\n",
    "\n",
    "def parse_geom_safe(wkt_string):\n",
    "    try:\n",
    "        if isinstance(wkt_string, str) and \"SRID=\" in wkt_string:\n",
    "            return wkt.loads(wkt_string.split(\";\", 1)[1])\n",
    "    except:\n",
    "        pass\n",
    "    return None  # simplemente ignora lo inválido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener y normalizar datos\n",
    "url = \"https://192.168.179.3/api/v1/poligonos_lotes/\"\n",
    "url_productor = \"https://192.168.179.3/api/v1/productor/\"\n",
    "url_productor_2 = \"https://192.168.179.3/api/v1/productor/mongo_get/productor_data/\"\n",
    "url_fincas = \"https://192.168.179.3/api/v1/poligonos_fincas/\"\n",
    "url_lotes = \"https://192.168.179.3/api/v1/poligonos_lotes/mongo_get/mongo_atribute/\"\n",
    "print(\"cargando datos lotes\")\n",
    "datos = get_data_paginated(url)\n",
    "print(\"cargando datos productor\")\n",
    "datos_productor = get_data_not_paginated(url_productor)\n",
    "print(\"cargando datos productor\")\n",
    "datos_productor_2 = get_data_not_paginated(url_productor_2)\n",
    "print(\"cargando datos fincas\")\n",
    "datos_fincas = get_data_paginated(url_fincas)\n",
    "print(\"cargando datos lotes\")\n",
    "datos_lotes = get_data_not_paginated(url_lotes)\n",
    "\n",
    "df = pd.json_normalize(datos)\n",
    "df_productor = pd.json_normalize(datos_productor)\n",
    "df_productor_2 = pd.json_normalize(datos_productor_2)\n",
    "df_fincas = pd.json_normalize(datos_fincas)\n",
    "df_lotes = pd.json_normalize(datos_lotes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar productores que tienen postgres_data.id\n",
    "df_productor = df_productor[df_productor[\"id\"].notna()]\n",
    "# Realizar los merge sin eliminar filas del DataFrame principal\n",
    "df_completo = df.merge(\n",
    "    df_productor,\n",
    "    left_on=\"productor\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "df_completo = df_completo.merge(\n",
    "    df_productor_2,\n",
    "    left_on=\"productor\",\n",
    "    right_on=\"postgres_data.id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_completo = df_completo.merge(\n",
    "    df_fincas,\n",
    "    left_on=\"finca\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_completo = df_completo.merge(\n",
    "    df_lotes,\n",
    "    left_on=\"id_x\",\n",
    "    right_on=\"poligono_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Aplicar solo a las válidas\n",
    "df_completo[\"geometry\"] = df_completo[\"poligono_x\"].apply(parse_geom_safe)\n",
    "\n",
    "# Crear el GeoDataFrame sin que explote\n",
    "gdf_lotes = gpd.GeoDataFrame(df_completo, geometry=\"geometry\", crs=\"EPSG:4326\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener y normalizar datos\n",
    "url = \"https://192.168.179.3/api/v1/poligonos_fincas/\"\n",
    "url_productor = \"https://192.168.179.3/api/v1/productor/\"\n",
    "url_productor_2 = \"https://192.168.179.3/api/v1/productor/mongo_get/productor_data/\"\n",
    "url_lotes = \"https://192.168.179.3/api/v1/poligonos_fincas/mongo_get/mongo_atribute/\"\n",
    "print(\"cargando datos fincas\")\n",
    "datos = get_data_paginated(url)\n",
    "print(\"cargando datos productor\")\n",
    "datos_productor = get_data_not_paginated(url_productor)\n",
    "print(\"cargando datos productor_2\")\n",
    "datos_productor_2 = get_data_not_paginated(url_productor_2)\n",
    "print(\"cargando datos fincas atributos\")\n",
    "datos_lotes = get_data_not_paginated(url_lotes)\n",
    "\n",
    "df = pd.json_normalize(datos)\n",
    "df_productor = pd.json_normalize(datos_productor)\n",
    "df_productor_2 = pd.json_normalize(datos_productor_2)\n",
    "df_lotes = pd.json_normalize(datos_lotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar productores que tienen postgres_data.id\n",
    "df_productor = df_productor[df_productor[\"id\"].notna()]\n",
    "# Realizar los merge sin eliminar filas del DataFrame principal\n",
    "df_completo = df.merge(\n",
    "    df_productor,\n",
    "    left_on=\"productor\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_completo = df_completo.merge(\n",
    "    df_productor_2,\n",
    "    left_on=\"productor\",\n",
    "    right_on=\"postgres_data.id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "df_completo = df_completo.merge(\n",
    "    df_lotes,\n",
    "    left_on=\"id_x\",\n",
    "    right_on=\"poligono_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Aplicar solo a las válidas\n",
    "df_completo[\"geometry\"] = df_completo[\"poligono\"].apply(parse_geom_safe)\n",
    "\n",
    "# Crear el GeoDataFrame sin que explote\n",
    "gdf = gpd.GeoDataFrame(df_completo, geometry=\"geometry\", crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"C:\\Users\\dorito\\Downloads\\HABILIDAD OCTUBRE 2025 COOCENTRAL(3.626) - UNIDAD TECNICA.xlsx\", sheet_name=\"OCTUBRE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTA_CEDULAS = df[\"CEDULA\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.loc[gdf[\"documento\"].isin(LISTA_CEDULAS)]\n",
    "gdf = gdf.sort_values(\"area\", ascending=False)\n",
    "gdf[\"geometry\"] = gdf[\"geometry\"].buffer(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_lotes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import unary_union\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Parámetros que puedes ajustar\n",
    "PROJ_CRS = 3116            # CRS métrico para área (usa el que sea correcto para tu región)\n",
    "MIN_INTERSECTION_RATIO = 0.70   # proporción mínima para considerar pertenencia\n",
    "MIN_ABS_INTERSECTION_AREA_M2 = 10  # área mínima absoluta en m² para descartar intersecciones puntuales/liniales\n",
    "SHRINK_EPS = 0.5           # metros para \"encoger\" polígonos y evitar contactos por vértice (ajustable)\n",
    "\n",
    "# --- Preprocesamiento: trabajar en proyección métrica para todas las operaciones de área/overlay\n",
    "gdf_proj = gdf.to_crs(PROJ_CRS).copy()\n",
    "gdf_lotes_proj = gdf_lotes.to_crs(PROJ_CRS).copy()\n",
    "\n",
    "# Para poder referir a atributos originales (si necesitas) mantén el gdf original también\n",
    "gdf_orig = gdf.copy()\n",
    "\n",
    "ids_evaluados = set()\n",
    "respuesta = []\n",
    "poligonos_fincas_duplicados = []\n",
    "counter = 1\n",
    "n = len(gdf_proj)\n",
    "\n",
    "for idx, row in gdf_proj.iterrows():\n",
    "    print(f\"{counter}/{n}\")\n",
    "    counter += 1\n",
    "\n",
    "    id_actual = row[\"id_x\"]\n",
    "    cedula_actual = row[\"documento\"]\n",
    "\n",
    "    if id_actual in ids_evaluados:\n",
    "        continue\n",
    "\n",
    "    poly_head = row[\"geometry\"]\n",
    "    if poly_head is None or poly_head.is_empty:\n",
    "        continue\n",
    "\n",
    "    # opcional: encoger un poco para evitar que contactos por vértice se consideren intersección real\n",
    "    try:\n",
    "        poly_head_shrunk = poly_head.buffer(-SHRINK_EPS)\n",
    "        if poly_head_shrunk.is_empty:\n",
    "            poly_head_shrunk = poly_head  # fallback si el shrink deja vacío\n",
    "    except Exception:\n",
    "        poly_head_shrunk = poly_head\n",
    "\n",
    "    area_head = poly_head.area  # en m²\n",
    "    if area_head <= 0:\n",
    "        continue\n",
    "\n",
    "    ids_familiares = []\n",
    "    ids_evaluados.add(id_actual)\n",
    "\n",
    "    # Recorremos otros polígonos (en la versión proyectada)\n",
    "    for idx2, row2 in gdf_proj.iterrows():\n",
    "        id2 = row2[\"id_x\"]\n",
    "        if id2 == id_actual or id2 in ids_evaluados:\n",
    "            continue\n",
    "\n",
    "        poly2 = row2[\"geometry\"]\n",
    "        if poly2 is None or poly2.is_empty:\n",
    "            continue\n",
    "\n",
    "        # también intentar shrink en el otro para evitar contactos puntuales\n",
    "        try:\n",
    "            poly2_shrunk = poly2.buffer(-SHRINK_EPS)\n",
    "            if poly2_shrunk.is_empty:\n",
    "                poly2_shrunk = poly2\n",
    "        except Exception:\n",
    "            poly2_shrunk = poly2\n",
    "\n",
    "        # Si no se intersectan geométricamente, skip rápido\n",
    "        if not poly_head_shrunk.intersects(poly2_shrunk):\n",
    "            continue\n",
    "\n",
    "        inter = poly_head_shrunk.intersection(poly2_shrunk)\n",
    "        if inter.is_empty:\n",
    "            continue\n",
    "\n",
    "        # Ignorar intersecciones que son solo punto o línea (contactos)\n",
    "        if inter.geom_type in (\"Point\", \"MultiPoint\", \"LineString\", \"MultiLineString\"):\n",
    "            continue\n",
    "\n",
    "        inter_area = inter.area  # en m²\n",
    "        # Descartar intersecciones demasiado pequeñas (tolerancia absoluta)\n",
    "        if inter_area < MIN_ABS_INTERSECTION_AREA_M2:\n",
    "            continue\n",
    "\n",
    "        # Proporción: comparo con el área del polígono *más pequeño* para evitar falsos si uno es mucho más grande\n",
    "        area_poly2 = poly2.area\n",
    "        denom = min(area_head, area_poly2) if min(area_head, area_poly2) > 0 else max(area_head, area_poly2)\n",
    "        pct = inter_area / denom\n",
    "\n",
    "        # Considerar también contains (pero usando geometrías no-shrunk para mayor robustez)\n",
    "        contains_check = poly_head.contains(poly2)\n",
    "\n",
    "        if pct >= MIN_INTERSECTION_RATIO or contains_check:\n",
    "            ids_evaluados.add(id2)\n",
    "            ids_familiares.append(id2)\n",
    "\n",
    "    # Procesar familia si se encontró al menos 1 miembro\n",
    "    if len(ids_familiares) > 0:\n",
    "        # Tomamos familiares que no sean la misma cédula (igual que antes)\n",
    "        gdf_familiares = gdf_orig.loc[(gdf_orig[\"id_x\"].isin(ids_familiares)) & (gdf_orig[\"documento\"] != cedula_actual)]\n",
    "\n",
    "        if len(gdf_familiares) == 0:\n",
    "            gdf_familiares = gdf_orig.loc[gdf_orig[\"id_x\"].isin(ids_familiares)]\n",
    "            gdf_familia_total = gpd.GeoDataFrame(pd.concat([gdf_orig.loc[gdf_orig[\"id_x\"]==id_actual], gdf_familiares]), crs=gdf_orig.crs)\n",
    "            poligonos_fincas_duplicados.append(gdf_familia_total)\n",
    "            continue\n",
    "\n",
    "        # Unificar cabeza + familiares (original CRS para almacenamiento)\n",
    "        gdf_cabeza_familiar = gdf_orig.loc[gdf_orig[\"id_x\"] == id_actual]\n",
    "        gdf_familia_total = gpd.GeoDataFrame(pd.concat([gdf_cabeza_familiar, gdf_familiares]), crs=gdf_orig.crs)\n",
    "\n",
    "        # Calcular área total de la familia en CRS métrico\n",
    "        gdf_familia_total_proj = gdf_familia_total.to_crs(PROJ_CRS)\n",
    "        poligono_total = unary_union(gdf_familia_total_proj[\"geometry\"])\n",
    "        area_total_ha = poligono_total.area / 10000  # hectáreas\n",
    "\n",
    "        # Deduplicar por documento y crear lista de miembros con índice limpio\n",
    "        gdf_familia_total_unique = gdf_familia_total.drop_duplicates(subset=\"documento\").reset_index(drop=True)\n",
    "\n",
    "        miembros_texto = \"\\n\".join(\n",
    "            f\"{i}. {doc} - {nom}\"\n",
    "            for i, (doc, nom) in enumerate(zip(gdf_familia_total_unique[\"documento\"], gdf_familia_total_unique[\"datos.nombre_completo\"]), start=1)\n",
    "        )\n",
    "\n",
    "        miembros = []\n",
    "        for idx_m, row_m in gdf_familia_total_unique.reset_index(drop=True).iterrows():\n",
    "            miembros.append({\n",
    "                \"indice\": idx_m + 1,\n",
    "                \"documento\": row_m[\"documento\"],\n",
    "                \"codigo_finca\": row_m.get(\"codigo_finca\", None)\n",
    "            })\n",
    "\n",
    "        # Construir estructura base\n",
    "        data_familia = {\n",
    "            \"cabeza_familia\": f\"{gdf_cabeza_familiar['documento'].iloc[0]} - {gdf_cabeza_familiar['datos.nombre_completo'].iloc[0]}\",\n",
    "            \"numero_productores_en_familia\": len(gdf_familia_total),\n",
    "            \"area_finca_familiar_ha\": area_total_ha,\n",
    "            \"miembros_familiares\": miembros_texto\n",
    "        }\n",
    "\n",
    "        # Para calcular el área por miembro usamos gdf_lotes_proj (CRS métrico)\n",
    "        df_miembros = pd.DataFrame(miembros)\n",
    "        for _, mrow in df_miembros.iterrows():\n",
    "            documento_familiar = mrow[\"documento\"]\n",
    "            codigo_finca = mrow[\"codigo_finca\"]\n",
    "\n",
    "            # Filtrado en proyectado\n",
    "            gdf_lotes_filtro = gdf_lotes_proj.loc[\n",
    "                (gdf_lotes_proj[\"documento\"] == documento_familiar) &\n",
    "                (gdf_lotes_proj[\"codigo_finca\"] == codigo_finca)\n",
    "            ]\n",
    "\n",
    "            if gdf_lotes_filtro.empty:\n",
    "                area_finca_ha = 0.0\n",
    "            else:\n",
    "                pol_miembro = unary_union(gdf_lotes_filtro.geometry)\n",
    "                if pol_miembro.is_empty:\n",
    "                    area_finca_ha = 0.0\n",
    "                else:\n",
    "                    area_finca_ha = pol_miembro.area / 10000  # hectáreas\n",
    "\n",
    "            # Nombre de columna claro y seguro\n",
    "            col_name = f\"miembro_{int(mrow['indice'])}_area_ha\"\n",
    "            data_familia[col_name] = area_finca_ha\n",
    "\n",
    "        respuesta.append(data_familia)\n",
    "\n",
    "print(\"Procesamiento finalizado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"./Familias Coocentral.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
