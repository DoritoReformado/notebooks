{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "import math\n",
    "import numpy\n",
    "from datetime import datetime\n",
    "from shapely import wkt\n",
    "from shapely.errors import WKTReadingError\n",
    "from shapely.geometry import mapping, shape\n",
    "from shapely import Polygon\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import numpy as np\n",
    "from shapely.validation import make_valid\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_base = \"./FELIPE ANDRES AROCA/\"\n",
    "url_base = \"https://pruebas.local/\"\n",
    "\n",
    "folder_individuales = os.path.join(folder_base, \"shp\")\n",
    "archivos_individuales = [os.path.join(folder_individuales, archivo) for archivo in os.listdir(folder_individuales) if archivo.endswith(\".shp\")]\n",
    "\n",
    "capas_gdf_dict = {\n",
    "    \"poligonos_fincas\": f\"{url_base}api/v1/poligonos_fincas/\",\n",
    "    \"poligonos_lotes\": f\"{url_base}api/v1/poligonos_lotes/\",\n",
    "    \"poligonos_infraestructura\": f\"{url_base}api/v1/poligonos_infraestructura/\",\n",
    "    \"poligonos_conservacion\": f\"{url_base}api/v1/poligonos_conservacion/\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "login = {\n",
    "    \"username\":\"Dorito\",\n",
    "    \"password\":\"Portador123\"\n",
    "}\n",
    "# Crear una sesión para mantener la cookie\n",
    "session = requests.Session()\n",
    "response = session.post(f\"{url_base}api/user/login/\", login, verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_paginated(url, url_base):\n",
    "    login = {\n",
    "        \"username\":\"Dorito\",\n",
    "        \"password\":\"Portador123\"\n",
    "    }\n",
    "    # Crear una sesión para mantener la cookie\n",
    "    session = requests.Session()\n",
    "    response = session.post(f\"{url_base}api/user/login/\", login, verify=False)\n",
    "    datos = []\n",
    "    while url:\n",
    "        response = session.get(url, verify=False)\n",
    "        info = response.json()\n",
    "        datos.extend(info[\"results\"])\n",
    "        url = info[\"next\"]\n",
    "    return datos\n",
    "\n",
    "def get_data_not_paginated(url, url_base):\n",
    "    login = {\n",
    "        \"username\":\"Dorito\",\n",
    "        \"password\":\"Portador123\"\n",
    "    }\n",
    "    # Crear una sesión para mantener la cookie\n",
    "    session = requests.Session()\n",
    "    response = session.post(f\"{url_base}api/user/login/\", login, verify=False)\n",
    "    response = session.get(url, verify=False)\n",
    "    info = response.json()\n",
    "    return info\n",
    "\n",
    "\n",
    "def parse_geom_safe(wkt_string):\n",
    "    try:\n",
    "        if isinstance(wkt_string, str) and \"SRID=\" in wkt_string:\n",
    "            return wkt.loads(wkt_string.split(\";\", 1)[1])\n",
    "    except:\n",
    "        pass\n",
    "    return None  # simplemente ignora lo inválido\n",
    "\n",
    "def obtener_gdf(origen, url_base):\n",
    "    if not origen == \"poligonos_fincas\":\n",
    "        url_fincas = f\"{url_base}api/v1/poligonos_fincas/\"\n",
    "        print(\"cargando datos fincas\")\n",
    "        datos_fincas = get_data_paginated(url_fincas, url_base)\n",
    "        df_fincas = pd.json_normalize(datos_fincas)\n",
    "        df_fincas = df_fincas[[\"id\",\"codigo_finca\"]]\n",
    "        df_fincas.columns = [\"identificador\", \"codigo_finca\"]\n",
    "\n",
    "    # Obtener y normalizar datos\n",
    "    url = f\"{url_base}api/v1/{origen}/\"\n",
    "    url_productor = f\"{url_base}api/v1/productor/\"\n",
    "    url_lotes = f\"{url_base}api/v1/{origen}/mongo_get/mongo_atribute/\"\n",
    "\n",
    "    print(f\"cargando datos {origen}\")\n",
    "    datos = get_data_paginated(url, url_base)\n",
    "    print(\"cargando datos productor\")\n",
    "    datos_productor = get_data_not_paginated(url_productor, url_base)\n",
    "    print(f\"cargando datos atributos {origen}\")\n",
    "    datos_lotes = get_data_not_paginated(url_lotes, url_base)\n",
    "\n",
    "    df = pd.json_normalize(datos)\n",
    "    df_productor = pd.json_normalize(datos_productor)\n",
    "    df_lotes = pd.json_normalize(datos_lotes)\n",
    "\n",
    "    # Filtrar productores que tienen postgres_data.id\n",
    "    df_productor = df_productor[df_productor[\"id\"].notna()]\n",
    "    df_productor = df_productor.rename(columns={\"id\":\"identificador_productor\"})\n",
    "    # Realizar los merge sin eliminar filas del DataFrame principal\n",
    "    df_completo = df.merge(\n",
    "        df_productor,\n",
    "        left_on=\"productor\",\n",
    "        right_on=\"identificador_productor\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    if not origen == \"poligonos_fincas\":\n",
    "        df_completo = df_completo.merge(\n",
    "            df_fincas,\n",
    "            left_on=\"finca\",\n",
    "            right_on=\"identificador\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    df_completo = df_completo.merge(\n",
    "        df_lotes,\n",
    "        left_on=\"id\",\n",
    "        right_on=\"poligono_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Aplicar solo a las válidas\n",
    "    df_completo[\"geometry\"] = df_completo[\"poligono\"].apply(parse_geom_safe)\n",
    "\n",
    "    # Crear el GeoDataFrame sin que explote\n",
    "    gdf = gpd.GeoDataFrame(df_completo, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for archivo in archivos_individuales:\n",
    "    nombre_elemento = os.path.basename(archivo)[:-4]\n",
    "    nombre_elemento = nombre_elemento.lower().replace(\" \", \"_\")\n",
    "    exec(f\"{nombre_elemento} = gpd.read_file('{archivo}')\")\n",
    "    print(f\"creada_instancia: {nombre_elemento}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "finca_del = finca.loc[finca[\"deshabilit\"] == 1]\n",
    "lote_del = lote.loc[lote[\"deshabilit\"] == 1]\n",
    "finca_act = finca.loc[finca[\"actualizar\"] == 1]\n",
    "lote_act = lote.loc[lote[\"actualizar\"] == 1]\n",
    "finca_add = finca.loc[finca[\"id\"] == '1']\n",
    "lote_add = lote.loc[lote[\"id\"] == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "finca_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_int(value, default=None):\n",
    "    \"\"\"\n",
    "    Convierte un valor a int de manera segura.\n",
    "    - Si es NaN, None o no convertible, devuelve `default`.\n",
    "    - Si es float convertible a entero, castea.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pd.isna(value):\n",
    "            return default\n",
    "        return int(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "    \n",
    "def subir_fotos(ruta_foto):\n",
    "    url_fotos = f\"{url_base}api/v1/subir-foto/\"\n",
    "    ruta_archivo = os.path.join(folder_base, ruta_foto)\n",
    "    with open(ruta_archivo, \"rb\") as f:\n",
    "        files = {\n",
    "            \"archivo\": (ruta_foto, f, \"image/jpeg\")  \n",
    "        }\n",
    "        response = requests.post(url_fotos, files=files, verify=False)\n",
    "    \n",
    "    if response.status_code == 201:\n",
    "        resultado = response.json()\n",
    "        return resultado[\"url\"]\n",
    "\n",
    "def limpiar_nan(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: limpiar_nan(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [limpiar_nan(v) for v in obj]\n",
    "    elif isinstance(obj, float) and (math.isnan(obj) or math.isinf(obj)):\n",
    "        return None\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def compiler_info_productor(nombre):\n",
    "    objeto_productor = {\n",
    "        \"activo\":False,\n",
    "        \"delegado\": False,\n",
    "        \"nombre_completo\": nombre,\n",
    "        \"fecha_afiliacion\": str(datetime.now().strftime(\"%Y-%m-%d\")),\n",
    "    }\n",
    "    return objeto_productor\n",
    "    \n",
    "\n",
    "def reportar_lotes(row):\n",
    "    if bool(row['otro_asoc']):\n",
    "        if not pd.isna(row[\"doc_prod\"]):\n",
    "            documento= safe_int(row[\"doc_prod\"])\n",
    "        else:\n",
    "            documento=None\n",
    "    else:\n",
    "        if not pd.isna(row[\"documento\"]):\n",
    "            documento= safe_int(row[\"documento\"])\n",
    "        else:\n",
    "            documento=None\n",
    "\n",
    "    response = {\n",
    "        \"documento_productor\":documento,\n",
    "        \"poligono\":str(row[\"geometry\"]),\n",
    "        \"mongo_atribute\":{\n",
    "            \"numero_documento\": str(documento),\n",
    "            \"nombre_productor\": str(row['nom_prod']),\n",
    "            \"fecha_visita\": pd.to_datetime(row['fecha_visi']).strftime(\"%Y-%m-%d\") if pd.notnull(row['fecha_visi']) else None,\n",
    "            \"area\": row['area'],\n",
    "            \"observaciones\": \"\",\n",
    "            \"descripcion\": \"\",\n",
    "            \"numero_lote\": row['numero_lot'],\n",
    "            \"descripcion_lote\": \"\",\n",
    "            \"variedad\": row['variedad'],\n",
    "            \"distancia_surcos\": row['distancia_'],\n",
    "            \"distancia_plantas\": row['distanci_1'],\n",
    "            \"densidad\": row['densidad'],\n",
    "            \"numero_plantas\": row['numero_pla'],\n",
    "            \"gramos_plantas\": row['gramos_pla'],\n",
    "            \"kg_produccion\": row['kg_producc'],\n",
    "            \"fecha_actividad\": pd.to_datetime(row['fecha_acti']).strftime(\"%Y-%m-%d\") if pd.notnull(row['fecha_acti']) else None,\n",
    "            \"produccion\": row['produccion'],\n",
    "            \"estado_cultivo\":row['estado_cul'],\n",
    "            \"subtipo_operacion\": row['subtipo_op']\n",
    "        },\n",
    "        \"productor_data\":{}\n",
    "    }\n",
    "\n",
    "    if bool(row['otro_asoc']):\n",
    "        if not pd.isna(row[\"nom_prod\"]):\n",
    "            nombre = str(row[\"nom_prod\"])\n",
    "            info_productor = compiler_info_productor(nombre)\n",
    "            response[\"productor_data\"] = info_productor\n",
    "\n",
    "    response = limpiar_nan(response)\n",
    "    return response\n",
    "\n",
    "def reportar_fincas(row):\n",
    "    if bool(row['otro_asoc']):\n",
    "        if not pd.isna(row[\"doc_prod\"]):\n",
    "            documento= safe_int(row[\"doc_prod\"])\n",
    "        else:\n",
    "            documento=None\n",
    "    else:\n",
    "        if not pd.isna(row[\"documento\"]):\n",
    "            documento= safe_int(row[\"documento\"])\n",
    "        else:\n",
    "            documento=None\n",
    "    response = {\n",
    "        \"documento_productor\":documento,\n",
    "        \"poligono\":str(row[\"geometry\"]),\n",
    "        \"mongo_atribute\":{\n",
    "            \"documento\":str(documento),\n",
    "            \"nombre_productor\":str(row[\"nom_prod\"]),\n",
    "            \"area\": row['area'],\n",
    "            \"nombre_finca\": row['nombre_fin'],\n",
    "            \"fecha_visita\": pd.to_datetime(row['fecha_visi']).strftime(\"%Y-%m-%d\") if pd.notnull(row['fecha_visi']) else None\n",
    "        },\n",
    "        \"productor_data\":{}\n",
    "    }\n",
    "    \n",
    "    if bool(row['otro_asoc']):\n",
    "        if not pd.isna(row[\"nom_prod\"]):\n",
    "            nombre = str(row[\"nom_prod\"])\n",
    "            info_productor = compiler_info_productor(nombre)\n",
    "            response[\"productor_data\"] = info_productor\n",
    "    \n",
    "    response = limpiar_nan(response)\n",
    "    return response\n",
    "\n",
    "def reportar_conservacion(row):\n",
    "    if bool(row['Asociado']):\n",
    "        if not pd.isna(row[\"doc_prod\"]):\n",
    "            documento= safe_int(row[\"doc_prod\"])\n",
    "        else:\n",
    "            documento=None\n",
    "    else:\n",
    "        if not pd.isna(row[\"doc_aso\"]):\n",
    "            documento= safe_int(row[\"doc_aso\"])\n",
    "        else:\n",
    "            documento=None\n",
    "\n",
    "    response = {\n",
    "        \"documento_productor\":documento,\n",
    "        \"poligono\":str(row[\"geometry\"]),\n",
    "        \"mongo_atribute\":{\n",
    "            \"documento\":str(documento),\n",
    "            \"nombre_productor\":str(row[\"nom_prod\"]),\n",
    "            \"area\": row['area'],\n",
    "            \"tipo_arboles\":row['tipo_arb'],\n",
    "            \"fecha_visita\": pd.to_datetime(row['fecha_vis']).strftime(\"%Y-%m-%d\") if pd.notnull(row['fecha_vis']) else None\n",
    "        },\n",
    "        \"productor_data\":{}\n",
    "    }\n",
    "\n",
    "    if bool(row['Asociado']):\n",
    "        if not pd.isna(row[\"nom_prod\"]):\n",
    "            nombre = str(row[\"nom_prod\"])\n",
    "            info_productor = compiler_info_productor(nombre)\n",
    "            response[\"productor_data\"] = info_productor\n",
    "    \n",
    "    response = limpiar_nan(response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "def reportar_infraestructura(row):\n",
    "    if bool(row['Asociado']):\n",
    "        if not pd.isna(row[\"doc_prod\"]):\n",
    "            documento= safe_int(row[\"doc_prod\"])\n",
    "        else:\n",
    "            documento=None\n",
    "    else:\n",
    "        if not pd.isna(row[\"doc_aso\"]):\n",
    "            documento= safe_int(row[\"doc_aso\"])\n",
    "        else:\n",
    "            documento=None\n",
    "    response = {\n",
    "        \"documento_productor\":documento,\n",
    "        \"poligono\":str(row[\"geometry\"]),\n",
    "        \"mongo_atribute\":{\n",
    "            \"documento\":str(documento),\n",
    "            \"nombre_productor\":str(row[\"nom_prod\"]),\n",
    "            \"area\": row['area'],\n",
    "            \"fecha_visita\": pd.to_datetime(row['fecha_vis']).strftime(\"%Y-%m-%d\") if pd.notnull(row['fecha_vis']) else None,\n",
    "            \"tipo_estructura\": row['tipo_estr'],\n",
    "            \"estructura\": row['estruc_sel']\n",
    "        },\n",
    "        \"productor_data\":{}\n",
    "    }\n",
    "\n",
    "    if bool(row['Asociado']):\n",
    "        if not pd.isna(row[\"nom_prod\"]):\n",
    "            nombre = str(row[\"nom_prod\"])\n",
    "            info_productor = compiler_info_productor(nombre)\n",
    "            response[\"productor_data\"] = info_productor\n",
    "\n",
    "    response = limpiar_nan(response)\n",
    "\n",
    "    return response\n",
    "\n",
    "def actualizar_fincas_existentes(row):\n",
    "    if bool(row['otro_asoc']):\n",
    "        if not pd.isna(row[\"doc_prod\"]):\n",
    "            documento= safe_int(row[\"doc_prod\"])\n",
    "        else:\n",
    "            documento=None\n",
    "    else:\n",
    "        if not pd.isna(row[\"documento\"]):\n",
    "            documento= safe_int(row[\"documento\"])\n",
    "        else:\n",
    "            documento=None\n",
    "    respuesta={\n",
    "        \"poligono\":str(row[\"geometry\"]),\n",
    "        \"mongo_atribute\":{\n",
    "            \"documento\":str(documento),\n",
    "            \"nombre_finca\":row[\"nombre_fin\"],\n",
    "            \"fecha_visita\":pd.to_datetime(row['fecha_visi']).strftime(\"%Y-%m-%d\") if pd.notnull(row['fecha_visi']) else None,\n",
    "            \"area\":row[\"area\"]\n",
    "        },\n",
    "        \"productor_data\":{},\n",
    "        \"documento_productor\": documento,\n",
    "    }\n",
    "    \n",
    "    if bool(row['otro_asoc']):\n",
    "        if not pd.isna(row[\"nom_prod\"]):\n",
    "            nombre = str(row[\"nom_prod\"])\n",
    "            info_productor = compiler_info_productor(nombre)\n",
    "            respuesta[\"productor_data\"] = info_productor\n",
    "\n",
    "    response=limpiar_nan(respuesta)\n",
    "    return response\n",
    "\n",
    "def actualizar_lotes_existentes(row):\n",
    "    if bool(row['otro_asoc']):\n",
    "        if not pd.isna(row[\"doc_prod\"]):\n",
    "            documento= safe_int(row[\"doc_prod\"])\n",
    "        else:\n",
    "            documento=None\n",
    "    else:\n",
    "        if not pd.isna(row[\"documento\"]):\n",
    "            documento= safe_int(row[\"documento\"])\n",
    "        else:\n",
    "            documento=None\n",
    "    respuesta = {\n",
    "        \"poligono\":str(row[\"geometry\"]),\n",
    "        \"mongo_atribute\":{\n",
    "            \"nombre_productor\": str(documento),\n",
    "            \"fecha_visita\": pd.to_datetime(row['fecha_visi']).strftime(\"%Y-%m-%d\") if pd.notnull(row['fecha_visi']) else None,\n",
    "            \"area\": row['area'],\n",
    "            \"observaciones\": \"\",\n",
    "            \"descripcion\": \"\",\n",
    "            \"numero_lote\": row['numero_lot'],\n",
    "            \"descripcion_lote\": \"\",\n",
    "            \"variedad\": row['variedad'],\n",
    "            \"distancia_surcos\": row['distancia_'],\n",
    "            \"distancia_plantas\": row['distanci_1'],\n",
    "            \"densidad\": row['densidad'],\n",
    "            \"numero_plantas\": row['numero_pla'],\n",
    "            \"gramos_plantas\": row['gramos_pla'],\n",
    "            \"kg_produccion\": row['kg_producc'],\n",
    "            \"fecha_actividad\": pd.to_datetime(row['fecha_acti']).strftime(\"%Y-%m-%d\") if pd.notnull(row['fecha_acti']) else None,\n",
    "            \"produccion\": row['produccion'],\n",
    "            \"estado_cultivo\":row['estado_cul'],\n",
    "            \"subtipo_operacion\": row['subtipo_op']\n",
    "        },\n",
    "        \"productor_data\":{},\n",
    "        \"documento_productor\": documento,\n",
    "    }\n",
    "    if bool(row['otro_asoc']):\n",
    "        if not pd.isna(row[\"nom_prod\"]):\n",
    "            nombre = str(row[\"nom_prod\"])\n",
    "            info_productor = compiler_info_productor(nombre)\n",
    "            respuesta[\"productor_data\"] = info_productor\n",
    "            \n",
    "    respuesta = limpiar_nan(respuesta)\n",
    "    \n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_productor(cedula, df_productor):\n",
    "    print(cedula)\n",
    "    # Convertir a entero de forma segura\n",
    "    cedula_int = int(cedula)\n",
    "\n",
    "    # Convertir la columna 'documento' a entero (ignorando errores)\n",
    "    df_productor[\"documento_int\"] = pd.to_numeric(df_productor[\"documento\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # Filtrar comparando como enteros\n",
    "    df_filtro = df_productor.loc[df_productor[\"documento_int\"] == cedula_int]\n",
    "\n",
    "    return len(df_filtro) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_json_fotos(row, cols_foto):\n",
    "    attachments_diccionario = {}\n",
    "    for columna in cols_foto:\n",
    "        valor = row[columna]\n",
    "        if pd.isna(valor):\n",
    "            continue\n",
    "        attachments_diccionario[columna] = subir_fotos(valor)\n",
    "    \n",
    "    return attachments_diccionario\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reporte_coronel(gdf, tipo, metodo_reporte):\n",
    "    respuestas_exitosas_lote = []\n",
    "    respuestas_no_exitosas_lote = []\n",
    "    cols_foto = [c for c in gdf.columns if c.lower().startswith(\"foto\")]\n",
    "    for i, row in gdf.iterrows():    \n",
    "        respuesta = metodo_reporte(row)\n",
    "        attachments = {}\n",
    "        if len(cols_foto) > 0:\n",
    "            attachments = preparar_json_fotos(row, cols_foto)\n",
    "        respuesta[\"attachments\"] = attachments\n",
    "        response = session.post(capas_gdf_dict[tipo], json=respuesta, verify=False)\n",
    "        if response.ok:\n",
    "            try:\n",
    "                data = response.json()\n",
    "                if \"id\" in data or data.get(\"status\") == \"success\":\n",
    "                    respuestas_exitosas_lote.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar JSON en fila {i} del tipo {tipo}: {e}\")\n",
    "        else:\n",
    "            respuestas_no_exitosas_lote.append(response.json())\n",
    "    df_exitoso = pd.DataFrame(respuestas_exitosas_lote)\n",
    "    df_no_exitoso = pd.DataFrame(respuestas_no_exitosas_lote)\n",
    "\n",
    "    with pd.ExcelWriter(os.path.join(folder_base,f'{tipo}.xlsx'), engine='openpyxl') as writer:\n",
    "        df_exitoso.to_excel(writer, sheet_name='rep_suc', index=False)\n",
    "        df_no_exitoso.to_excel(writer, sheet_name='rep_not_suc', index=False)\n",
    "\n",
    "\n",
    "def actualizacion_coronel(gdf, tipo, metodo_reporte):\n",
    "    respuestas_exitosas_lote = []\n",
    "    respuestas_no_exitosas_lote = []\n",
    "    cols_foto = [c for c in gdf.columns if c.lower().startswith(\"foto\")]\n",
    "    gdf = gdf.loc[gdf[\"actualizar\"] == 1]\n",
    "    url = capas_gdf_dict[tipo]\n",
    "    gdf = gdf.rename(columns={'id_1': 'id'})\n",
    "    for i, row in gdf.iterrows():\n",
    "        if pd.isna(row[\"id\"]):\n",
    "            respuestas_exitosas_lote.append({\"objeto\":row, \"razon\": \"lote creado en capa erronea\"})\n",
    "        else:\n",
    "            fecha_ahora = datetime.now()\n",
    "            fecha_formateada = fecha_ahora.strftime(\"%d-%m-%Y_%H--%M--%S\")\n",
    "            url_actualizacion = f\"{url}update/mongo_update/{row['id']}/{fecha_formateada}/\"\n",
    "            respuesta = metodo_reporte(row)\n",
    "            documento = respuesta[\"documento_productor\"]\n",
    "            productor_data = respuesta[\"productor_data\"]\n",
    "            response = session.post(url_actualizacion, json=respuesta, verify=False)\n",
    "\n",
    "            url_actualizacion = f\"{url}{row['id']}/\"\n",
    "            respuesta = {\"poligono\": str(row[\"geometry\"])}\n",
    "            if documento is not None:\n",
    "                respuesta[\"documento_productor\"] = int(documento)\n",
    "                respuesta[\"productor_data\"] = productor_data\n",
    "\n",
    "            attachments = {}\n",
    "            if len(cols_foto) > 0:\n",
    "                attachments = preparar_json_fotos(row, cols_foto)\n",
    "            respuesta[\"attachments\"] = attachments\n",
    "            response = session.patch(url_actualizacion, json=respuesta, verify=False)\n",
    "            respuesta[\"id\"] = row[\"id\"]\n",
    "            if response.ok:\n",
    "                respuestas_exitosas_lote.append(respuesta)\n",
    "            else:\n",
    "                respuestas_no_exitosas_lote.append({\"id\":row['id'],\"respuesta\":respuesta})\n",
    "    df_exitoso = pd.DataFrame(respuestas_exitosas_lote)\n",
    "    df_no_exitoso = pd.DataFrame(respuestas_no_exitosas_lote)\n",
    "\n",
    "    with pd.ExcelWriter(os.path.join(folder_base,f'{tipo}_act.xlsx'), engine='openpyxl') as writer:\n",
    "        df_exitoso.to_excel(writer, sheet_name='rep_suc', index=False)\n",
    "        df_no_exitoso.to_excel(writer, sheet_name='rep_not_suc', index=False)\n",
    "\n",
    "def eliminacion_coronel(gdf, tipo):\n",
    "    respuestas_exitosas_lote = []\n",
    "    respuestas_no_exitosas_lote = []\n",
    "    for i, row in gdf.iterrows():\n",
    "        if row[\"id\"] != 1 or row[\"id\"] != '1':\n",
    "            response = session.delete(f\"{url_base}api/v1/{tipo}/{row['id']}\", verify=False)\n",
    "            if response.ok:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                    # Solo guardar si la respuesta incluye un ID o estado positivo\n",
    "                    if \"id\" in data or data.get(\"status\") == \"success\":\n",
    "                        respuestas_exitosas_lote.append(data)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al procesar JSON en fila {i} del tipo {tipo}: {e}\")\n",
    "            else:\n",
    "                respuestas_no_exitosas_lote.append(response.json())\n",
    "    df_exitoso = pd.DataFrame(respuestas_exitosas_lote)\n",
    "    df_no_exitoso = pd.DataFrame(respuestas_no_exitosas_lote)\n",
    "\n",
    "    with pd.ExcelWriter(os.path.join(folder_base,f'{tipo}_eliminados.xlsx'), engine='openpyxl') as writer:\n",
    "        df_exitoso.to_excel(writer, sheet_name='rep_suc', index=False)\n",
    "        df_no_exitoso.to_excel(writer, sheet_name='rep_not_suc', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "finca_add.to_file(os.path.join(folder_base,\"fincas_a_adicionar.shp\"))\n",
    "lote_add.to_file(os.path.join(folder_base,\"lotes_a_adicionar.shp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminacion_coronel(finca_del, \"poligonos_fincas\")\n",
    "eliminacion_coronel(lote_del, \"poligonos_lotes\")\n",
    "reporte_coronel(finca_add, \"poligonos_fincas\", reportar_fincas)\n",
    "reporte_coronel(lote_add, \"poligonos_lotes\", reportar_lotes)\n",
    "reporte_coronel(conservacion, \"poligonos_conservacion\", reportar_conservacion)\n",
    "reporte_coronel(construcciones, \"poligonos_infraestructura\", reportar_infraestructura)\n",
    "actualizacion_coronel(finca_act, \"poligonos_fincas\", actualizar_fincas_existentes)\n",
    "actualizacion_coronel(lote_act, \"poligonos_lotes\", actualizar_lotes_existentes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
